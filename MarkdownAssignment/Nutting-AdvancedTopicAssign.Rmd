---
title: "Advanced Topic"
author: "Andy Nutting"
date: "3/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##My problem (Everyone's problem)

  Idaho is a rural state with only 14% (6 of 44) of counties classified as urban. Counties provide a broad range of services and are recognized as general-purpose government entities. Many of the services provided by counties are mandates from the state. However, the primary source of funding for county and city governments in Idaho is revenue from property tax levies. 
 
  ##Some Idaho counties don't have a lot of people. Counties rely heavily on property taxes. Not a lot of people = lower revenue potential
  
  The purpose of this analysis is to explore the effect of tax and value changes in urban and rural counties in Idaho during 2018 and 2019. The impact of changes in taxes on existing properties is of particular interest with the goal of identifying concerns shared by urban and rural residents.   

##Policies are passed in Boise (which is Urban) and affect the whole state. Are these issues state wide?

Advance Topic: Study Design and Spatial Simulation

Current Study Design: Descriptive

  Property tax data used in this report is from the Idaho State Tax Commission. Records maintained by the Commission are compiled from reports submitted by each county and associated taxing districts. Information on the value of new construction is also from reports generated by counties and provided to the Commission.  
  A purposive sample of counties representing urban and rural characteristics was selected for this report.  Selection criteria included population size (urban and rural) and availability of data on new construction in the county. The following urban and rural counties met the selection criteria:

Urban:  Ada, Bonneville, Canyon, Kootenai, Twin Falls
Rural: Camas, Gooding, Custer, Lincoln

  In an attempt to capture a snapshot of the economic effects of current policies surrounding property taxes, we analyzed tax data for 2018 and 2019. We wanted to determine the average percent change in taxes on existing properties over the years, by property type, aggregated by counties with “High Proportion” Owner Occupied (OOC) and “Low Proportion” OOC, in order to depict the shift of the burden of property taxes across the state of Idaho. This was done by using Owner Occupied (OOC), Non-Owner Occupied (NOOOC), and Commercial Taxable Values (with Homeowner’s Exemption) from “AVTAX” reports from 2017-2019, as well as information from “Estimated New Construction” reports from 2017-2019, and information from “Market Values and Property Tax” reports from 2017-2019.  

(((100000*(% change in taxable value of the existing property)+100000)*(Averaged property tax rate by county))-(100000*(State Property tax rate of the previous year)))/(100000*(Averaged property tax rate of the prev
ious year)) 

##This equals the total average property tax change, accounting for new property, for any given county. 

Urban Vs Rural

The sum of all Urban/Rural counties (The total property value * the average tax rate accounting for new construction) / the sum of all Urban/Rural counties total property value.  

Simulation: Lets assume population numbers drive property values. Simulate population and how it changes property values given tax rates. What should I see in Idaho?

Goal: Analyze data to provide information to Idaho's Property Tax working Group to facilitate data backed decisions concerning policies surrounding property taxes. Goal of the simulation is to simulate outcomes given various contstraints

Lets get started...

## Introduce the packages
1) Where is the critical step?

Weighted matrix

2) What packages and functions are you considering to help you complete this step? 
tidyverse
ipfp for spatial simulation of population

Package ipfp: Iterative Proportional Fitting procedure (IPF). A simple, fast and widely-used method to allocate individuals to zones.

3) Why did you choose them? 

Because it was useful for my simulation and what I found in a book.

```{r}
#load the libraries
library(tidyverse)
library(ipfp)

#load your data - which datasets do you need?
setwd("C:/Users/andyj/Desktop/spatial-microsim-book-master/spatial-microsim-book-master")

#'Individual' data
ind <- read.csv("C:/Users/andyj/Desktop/drive-download-20200309T180645Z-001/survey.csv") 

#Contraints
con_income <- read.csv("C:/Users/andyj/Desktop/drive-download-20200309T180645Z-001/income.csv")
con_rate <- read.csv("C:/Users/andyj/Desktop/drive-download-20200309T180645Z-001/rate.csv")

class(ind) # verify the data type of the object
ind # print the individual-level data

ind_orig <- ind #for later

#Organize the data
# Convert age into a categorical variable with user-chosen labels

(ind$income <- cut(ind$income, breaks = c(0, 79000, 1000000), labels = c("i79k", "i80k+")))
  names(con_income) <- levels(ind$income) # rename aggregate variables
cons <- cbind(con_income, con_rate)

#flatten
cat_income <- model.matrix(~ ind$income - 1)
cat_rate <- model.matrix(~ ind$rate - 1)[, c(2, 1)]
(ind_cat <- cbind(cat_income, cat_rate)) # combine flat representations of the data

colSums(ind_cat) # view the aggregated version of ind
ind_agg <- colSums(ind_cat) # save the result

rbind(cons[1,], ind_agg) # test compatibility between ind_agg and cons objects

#The purpose of the reweighting in spatial microsimulation is to minimise this difference (as measured in TAE above) by adding high weights to the most representative individuals.
```


Step 1: Weight Matrix (Weighting)

```{r}
#Create weight matrix

weights <- matrix(data = NA, nrow = nrow(ind), ncol = nrow(cons))
dim(weights) # the dimension of the weight matrix: 5 rows by 3 columns

#Weighting Algorithms

cons <- apply(cons, 2, as.numeric) # convert matrix to numeric data type
ind_catt <- t(ind_cat) # save transposed version of ind_cat
x0 <- rep(1, nrow(ind)) # save the initial vector
weights <- apply(cons, 1, function(x) ipfp(x, ind_catt, x0, maxit = 20))

#Randomly allocate the 5 individuals of the microdata to a zone

# set the seed for reproducibility
set.seed(1) 
# create selection
sel <- sample(x = 5, size = 12, replace = T)
ind_z1 <- ind_orig[sel, ]
head(ind_z1, 3)

```

Step 2: Perform Interative Proportional Fitting Long Way (reweighting)

```{r}
#Easy naming
n_zone <- nrow(cons) # number of zones
n_ind <- nrow(ind) # number of individuals
n_income <-ncol(con_income) # number of categories of "income"
n_rate <-ncol(con_rate) # number of categories of "rate"

# Create initial matrix of categorical counts from ind 
weights <- matrix(data = 1, nrow = nrow(ind), ncol = nrow(cons))
# create additional weight objects
weights1 <- weights2 <- weights 
ind_agg0 <- t(apply(cons, 1, function(x) 1 * ind_agg))
colnames(ind_agg0) <- names(cons)

# Assign values to the previously created weight matrix 
# to adapt to income constraint
for(j in 1:n_zone){
  for(i in 1:n_income){
    index <- ind_cat[, i] == 1
    weights1[index, j] <- weights[index, j] * con_income[j, i] 
    weights1[index, j] <- weights1[index, j] / ind_agg0[j, i]
  }
  print(weights1)
}

#re-aggregate the results from individual level data after they have been reweighted

# Create additional ind_agg objects
ind_agg2 <- ind_agg1 <- ind_agg0 * NA

# Assign values to the aggregated data after con 1
for(i in 1:n_zone){
  ind_agg1[i, ] <- colSums(ind_cat * weights1[, i])
}

#Check work
rowSums(ind_agg1[, 1:2]) # the simulated populations in each zone

rowSums(cons[, 1:2]) # the observed populations in each zone

#goodness of fit
vec <- function(x) as.numeric(as.matrix(x))
cor(vec(ind_agg0), vec(cons))

cor(vec(ind_agg1), vec(cons))

#Check weights
weights1[, 1]
cons[1, ]

#constrain by rate
for(j in 1:n_zone){
  for(i in 1:n_rate + n_income){
    index <- ind_cat[, i] == 1
    weights2[index, j] <- weights1[index, j] * cons[j , i] /
      ind_agg1[j, i]
  }
}

weights2
for(i in 1:n_zone){
  ind_agg2[i, ] <- colSums(ind_cat * weights2[, i])
}

ind_agg2

```

The concept of IPF is to repeat this procedure several times. Thus, each iteration contains a re-constraining for each variable.

Step 3: IPFP package

```{r}
cons <- apply(cons, 2, as.numeric) # to 1d numeric data type
ipfp(cons[1,], t(ind_cat), x0 = rep(1, n_ind)) # run IPF

#Increase number of iterations
ipfp(cons[1,], t(ind_cat), rep(1, n_ind), maxit = 20, v = T)
```

Step 4: What was faster?

IPF with ipfp package

## Evaluate your choices

Its fast enough

## Show us your final product

```{r}


```


## Reflect
How has your skill improved? What do you wish you understood better? What do you imagine your next steps to be?

My skills have remained stagnant, the only thing to change is my understanding that I know very little. I wish I understood everything better. I imagine my next steps would be to spend eons reading books on spatial analysis in R. 
