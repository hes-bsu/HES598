---
title: "Advanced Topic: Landscape Metrics and Raster Data"
author: "Merry Davidson"
date: "4/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rgdal)
library(raster)
```


SAVE YOUR FILE IN THE AdvancedTopics FOLDER IN THE GIT REPO

## Introduction:
My thesis research aims to quantify the degree of landscape change on the landscape. The study site of interest is the Rupununi savanna-forest boundary located on the Southwest portion of Guyana (South America). In order to quantify change, I have generated rasters using Spectral Unmixing Analysis (SUA) classification for the period of 2000-2018.
My chosen Advanced Topic for this course is: Landscape Metrics and Raster Data. 
My goal for this assignment is to learn how to use raster data in R by using raster packages and fucntions [Specify wich ones here}}}. Specifically, I will obtain SUA pixel totals of 13 specific points in a subset of my study area.  
Bigger picture: This inforation will be used in a statistical generalized linear model that will look at wildlife-landscape interactions.



## PSUEDOCODE:


```{r}
#load the libraries - tell us which packages you're using and why
##Both raster packages to help format my 2 rasters  
## library(rgdal)
## library(raster)


#load your data - which datasets do you need?
##Dadanawa raster file Landsat 7, year 2015 greenest pixel composite image "DADYR15"
#DADYR15 <- raster()
##The SUA classification of year 2015 "SUADAD15"
#SUADAD15 <- raster()
##GPS points to get SUA totals for "DADPTS"
#DADPTS <- read.csv("C:/Users/Nessa/Desktop/BSU/RESEARCH/GUYresearch/Camera_trap/DAD1/DadAnalysisDAD1_GPSPoints.csv")


#Organize the data - what form should the data be in? A list? how many elements, a data frame? how many rows and columns?
##Make sure the two rasters stack up

#Analysis/processing step 1 - what are you hoping to do here, why? 
#Quantify the metrics by landcover type: water, forest, bareground and quantify the amount of patches per class

#Analysis/processing step 2 - what are you hoping to do here, why? 
#Are there differences in SUA values between the points? 9 out of the 13 points have been classified as bush island habitats. Is there more resolution or variability using SUA values instead of the catergorical habitat description?

#Check the outcomes? How will you know if your steps worked?
#If I have a SUA value for 13 points I will know I did it correctly.

#
```

## Introduce the packages
Given your psuedo code, where is the critical step? What packages and functions are you considering to help you complete this step? Why did you choose them? 
library(googledrive)
#for loading in the data
## library(rgdal)
## library(raster)

#Actual code:
Load data

```{r}
#for loading in the data
## 
library(googledrive) #where data is stored
# designate project-specific cache
options(gargle_oauth_cache = ".merrymd")
# check the value of the option, if you like
gargle::gargle_oauth_cache()
# trigger auth on purpose --> store a token in the specified cache
drive_auth()
###Run the above one time,done

library(googledrive)
#authenticating google drive
options(
  gargle_oauth_cache = ".merrymd",
  gargle_oauth_email = TRUE
)


library(curl) #called for Google Drive data access
library(here) #called for Google Drive data access (makes a temp folder for downloads)
library(readxl) #reads Excel files (how the data is stored)


folder_url <- "https://drive.google.com/open?id=1Co7q8ggqr1CjbdUcbN0-9LtnHygshKkq"

folder <- drive_get(as_id(folder_url))

files <- drive_ls(folder)
str(files)

lapply(files$id, function(x) drive_download(as_id(x), path = paste0(here::here("datatemp/original/"), files[files$id==x,]$name), overwrite = TRUE))



#"/path/to/your/local/directory/not/class repo/" overwrite = TRUE)) #replace the path with a folder on your computer outside of the repo directory

###file.list <- list.files((pattern = "*.tif") #replace the pattern with the appropriate extensions for the files you want to load (e.g., .shp, .csv)

###rstrs <- lapply(file.list raster) #Then read in the file using the appropriate read command (e.g., raster, read_sf, read.csv)

```




## Evaluate your choices
Use profiling and benchmarking to evaluate which of your options is likely to be the fastest. How does the syntax and/or ease of use of that function impact your decision of whether or not to use it? (For example, velox is much faster than raster, but it's less well documented and the syntax is strange to get used to).

## Show us your final product
Did you make a map? Let's see it. Did you plot some data that you extracted with raster? show us that plot. Did you have an idea of how the data should look after you were done processing it? Were you successful? What went wrong

## Reflect
Write a few sentences on what you learned from this exercise. How has your skill improved? What do you wish you understood better? What do you imagine your next steps to be?

Once you're done push the "knit" button to create the html page from your Rmarkdown document. If you've got questions, let me know!!