---
title: "Advanced Topic Instructions"
author: "Louis Jochems"
date: "3/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


SAVE YOUR FILE IN THE AdvancedTopics FOLDER IN THE GIT REPO

## Introduce your problem

Give a brief introduction to your research and identify how your "Advanced Topic" plays into what you are hoping to do. For example, if you are just starting out on your project and your topic is "Study Design" you might be interested in developing a spatially informed sampling/stratification scheme.Or maybe your work is going to have a big public outreach component that you'd like to support with some nice interactive maps. Give us a sense for what "success" would look like - what is your ideal end product for the analysis/approach you're testing out. This will help us figure out if the tools you tried were successful.

This chapter of my dissertation involves detecting and predicting the spread (or really, the range/distribution) of a newly invading macrophyte (Hydrocharis morsus-ranae L; EFB) in Great Lakes wetlands. Because EFB is not yet fully established in all Great Lakes, I am investigating whether we can accurately predict vulnerable wetlands based on EFB's environmental/anthropogenic drivers. The importance of these hypothesized drivers (eg. distance to public boat launch, the presence of an emergent macrophyte, etc.) can be assessed through various species distribution models. A species distribution model can have varying definitions, but I interpret it as a representation of the influence of ecological, climatic, and anthropogenic variables on whether or not a species' presence/abundance is observed across time and space. In other words, understanding which variables are important for EFB occurence at its known locations can help us predict the full potential range or habitat of EFB (based on the similarity of uninvaded wetlands variables to sites of known occurence) if it were to invade the entire GL basin. Overall, these questions are important to address so that managers can properly control/mitigate EFB's spread and priortize protection of vulnerable wetlands to future invasions. Spatio-temporal data play a huge role in answering my questions because predicting EFB's current and potential distribution in its invasive range requires spatial data ideally over decadal timescales.

Specifically for my research, I have a obtained access to a relatively long time-series field dataset, consisting of 8 years of transect vegetaion survey data in Michigan coastal wetlands (2011-2018). It includes plant species occurence/abundance, environmental data (water depth, distance to wave action, water quality, etc.), GPS coordinates of quadrats, and date at which the data was collected. I will also include a distance to boat launch as a proxy for human activity in these wetlands (and perhaps others?). Becuase there is a spatio-temporal component in these data, I hope to use the integrated nested laplace approximation (INLA) to account for inherent spatio-temporal autocorellation in this dataset as an error term (in the model (to account for non-independence of model residuals) and assess its relative importance compared to the othe potential drivers on EFB's distribution. 

For this advanced topic, I'll consider this exercise a success by cleaning/wrangling the dataset as it was given to me, and then exploring these data in various ways to determine whether or not an INLA framework will be appropriate for modeling EFB's distribution in Michigan wetlands. For example, I will first plot the presence/abundance data against the various predictors to examine which ones may influence EFB in some way (postive, negative, or quadratic). Second, I could then include some geographic attribute of each of these points to understand if there are autocorrelated points in these raw data (I could also do the same he data over time). Once I explore all of my variables of interest, I plan to map the most intresting ones (determined qualitatively) to get an ultimate sense of visualizing EFB distribution in relation to these variables. As an aspiration, I plan to run a preliminary model on some of these predictors on EFB's presence (and/or abundance), at least with a spatial autocorellation term from the INLA package. 

## PSUEDOCODE!!
Before you get started programming all of the potential things you could do during the course of this demo. Lay out the steps you'll need to get there (see below for what I mean). You don't have to actually code these things (yet) just help us see how you're approaching the problem.
```{r}
#load the libraries - tell us which packages you're using and why
knitr::opts_chunk$set(echo = TRUE)
library(googledrive)
# To connect to folder with field data. 
library(tidyverse)
library(reshape2)
#I need tidyverse and reshape2 so that I can rearrange (via the melt function) my dataset into a format that is analyzable. Specifically, the field data that was given to me consisted of individual species observations, each as their own row (so like, >70k rows!). Instead, I would like each row to be a given quadrat plot with all spp observed and recorded variables. tidyr's spread() function will help get the dataframe into the desired format. 

library(sf)
library(sp)
library(rgdal)
library(raster)
#These libraries help me eventually turn my dataframe with lat/lons into a .shp file that I'll eventually plot all over the state of MI. Each point is a surveyed quadrat in a given wetland. 

library(maps)
library(mapproj)
library(tmap)
library(maptools)
#These packages have functions to ensure my various shapefiles have the same projection, and can make my maps look pretty! 

library(ggplot2)
library(patchwork)
library(geofacet)
#ggplot2 gives me the flexibility to make my plots more informative (see above about including multiple attributes), and patchwork will potentially allow me to plot multiple panel plots to compare and contrast patterns across different variables. Lastly, geofacet will help me create plots of processes organized by different geographic subsets in the state (county, great lake, geomorphic subset etc)

library(RColorBrewer)
library(latticeExtra)
library(RColorBrewer)
library(viridis)
#library(rJava)
#For making my plots looks look pretty with nice color palletes. Might be useful for some of my variables that are on different scales. 

#(INLA)
#library(inlabru)
library(dismo)
#Finally, INLA allows me to create a species distribuion model (in a Bayesian framework) with a spatial/temporal autocorrelation terms relative to the effects of the hypothesized predictor variables. I can also plot the spatio-temporal effects of the variables across MI coastlines (ie. where/when EFB occurence exhibits the highest spatio-temporal autocorrelation). Eventually (though probably not in the scope of this exercise), I would like to plot the risk of uninvaded wetlands to potential EFB establishment. This would be based on the model estimates of the predictor's posterior distributions. 

library(microbenchmark) 
library(parallel)
#Lastly, although the INLA framework allows for high computation of variables with complex structure through stochastic partial differential equations, I still likely need to assess how long these models will take to run given this relatively long time-series of field data. 


#load your data - which datasets do you need?
setwd("Z:/SDMData/FullVegDataset2011-18")
veg_raw <- read.csv("FullVegDataset2011-18.csv")
m.raw <- melt(veg_raw, id=c("site_id","county","date","year","site_name","transect_num","meadow_width_m","emergent_width_m","submergent_width_m",
                            "point_num","gps_lon","gps_lat","water_depth_cm","bottom_vis","substrate_type","org_depth_cm","unvegetated_pcnt",
                            "total_cover_pcnt","standing_dead_pcnt","detritus_pcnt", "taxa_name"),
              measure.vars = c("percent_cover"))

m.raw <- m.raw[m.raw$taxa_name %in% c("Hydrocharis morsus-ranae","Typha angustifolia","Typha glauca","Typha glauca (hybrid)","Typha latifolia","Typha sp.",
                                   "Phragmites australis","Phragmites australis (invasive","Phragmites australis (native)"),]
#this is the full field dataset of vegetation surveys of Michigan wetlands from 2011-2018. This includes many spp observations, but I've reduced it to EFB, Typha, and Phragmites. 

#Organize the data - what form should the data be in? A list? how many elements, a data frame? how many rows and columns?
veg_data <- m.raw %>% spread(taxa_name, value)
#replace NA's with o 
veg_data[is.na(veg_data)]=0

#confusion on typha glauca and hybrid, so could combine them for now? 
veg_data$typha_combined <- veg_data$`Typha glauca` + veg_data$`Typha glauca (hybrid)` + veg_data$`Typha sp.`
veg_data <- data.frame(veg_data)

#create binomial presence/absence data (needed for hurdle model down the road)
veg_data$hyd_bin <- ifelse(veg_data$Hydrocharis.morsus.ranae>0,1,0)
veg_data$typh_bin <- ifelse(veg_data$typha_combined>0,1,0)

dim(veg_data)
#[1] 2255   30
#we have 2,225 rows with 30 columns of data. Each row is a quadrat from a given transect in a given year between 2011-18. 

##now I need to load in shapefiles so that I could calculate additional, potentially important predictors, e.g. fetch 
setwd("Z:/SDMData/RSpatialFiles")
MI_boundary <- st_read("Z:/SDMData/RSpatialFiles/MichiganBoundaries.shp")#state boundary layer 
GL_coastlines <- st_read("Z:/SDMData/RSpatialFiles/GreatLakesCoastlines.shp") #coastline layer useful for calculated fetch
wetland_polys <- st_read("Z:/SDMData/RSpatialFiles/great_lakes_coastal_wetlands.shp") #delineated wetlands in GL 
CWMP_surveys <- st_read("Z:/SDMData/RSpatialFiles/CWMPSurveys_EFB2011-18.shp") #centroid layer of all surveyed wetlands in time period
invaded_sites <- st_read("Z:/SDMData/RSpatialFiles/MI_InvadedSites2011_18.shp") #centroid layer of EFB-invaded sites only 


#Analysis/processing step 1 - what are you hoping to do here, why? 
#First, I need to create a shape file for my full quadrat layer 
#for now, subset to quads that have coords 
veg_data <- veg_data[which(veg_data$gps_lon < 0 & veg_data$gps_lat>0),]
#many quadrats without GPS coordinates 
#now only 1873 plots total

# now need to create .shp file
veg_data <- SpatialPointsDataFrame(veg_data[,11:12],
                                    veg_data)
crs(veg_data) <- "+proj=utm +zone=16 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
plot(veg_data)
```

```{r} 
#Analysis/processing step 2 - what are you hoping to do here, why? 
#for INLA purposes, let's subset data to just 2018 (most recent year of data)
#will load in subsetted data from class's google drive 
folder.url <- "https://drive.google.com/open?id=1cjqJMM2JyQWDtVorNEyO6HLprB_s7XOF" #replace this link with the link to your specific folder which you can get by clicking the "get shareable link option"

folder <- drive_get(as_id(folder.url))

files <- drive_ls(folder)

lapply(files$id, function(x) drive_download(as_id(x), path = "~/Rspatial/", overwrite = TRUE)) #replace the path with a folder on your computer outside of the repo directory

file.list <- list.files((pattern = "*.csv")) #replace the pattern with the appropriate extensions for the files you want to load (e.g., .shp, .csv)

field_data <- lapply(file.list, read.csv) #Then read in the file using the appropriate read command (e.g., raster, read_sf, read.csv)

#for dem in folder as well? 
file.list <- list.files((pattern = "*.tif")) 
raster <- lapply(filelist,raster)
```

```{r}
#need to subset to counties with records that overlap our topobathymetry layer
#will work with Munuscong Bay layer since it's the most contiguous 
UP_data <- veg_data[which(veg_data$county == "Chippewa"),]
UP_data <- as.data.frame(UP_data)

#check for duplicates of GPS coordinates 
dups <- UP_data[duplicated(UP_data[,c(11,12)]),c(11,12)]
#none!
#also see if there are no errors with gps coords 
range(UP_data$gps_lon)
range(UP_data$gps_lat)

#since our lidar layer is even a smaller subset of the area, we need to further subset points to ROI, around Munuscong Bay on the St. Mary's River (technically lake huron)
#will do so by site names, determined from GIS software
UP_data$site_name <- as.factor(UP_data$site_name)
Mcong <- UP_data[UP_data$site_name %in% c("Benchmark: Munuscong Island Wetland", "Benchmark: Munuscong Lake Wetland #6","Gogomain River Wetland #1","Munuscong Lake Wetland #2,#3 Munuscong River Delta","Raber Bay Wetland" ,"Roach Point Wetland"),]
within(Mcong, rm(gps_lon.1, gps_lat.1))

#let's make dataframe into spatial object with binomial occurence 
# now need to create .shp file
Mcong <- SpatialPointsDataFrame(Mcong[,11:12],
                                    Mcong)
crs(Mcong) <- "+proj=utm +zone=16 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
plot(Mcong)

```

```{r}
#loading in raster 
setwd("Z:/SDMData/GreatLakesLayers/Rasters")
topobath <- raster("Job534911_2013_2015_usace_huron_dem.tif")

#the two layers in different coordinate systems, I will reset topo layer to CRS of points layer 
#crs(topobath) <- "+proj=utm +zone=16 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
extent(topobath) <- Mcong

#or opposite? 
crs(Mcong) <- "+proj=lcc +lat_1=47.08333333333334 +lat_2=45.48333333333333 +lat_0=44.78333333333333 +lon_0=-87 +x_0=7999999.999968001 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=ft +no_defs"


plot(topobath)
points(Mcong,cex=0.5)
```

```{r}
#extract values from topobath layer to serve as a predictor variable for EFB presence 


```


```{r}
#Check the outcomes? How will you know if your steps worked?

```

## Introduce the packages
Given your psuedo code, where is the critical step? What packages and functions are you considering to help you complete this step? Why did you choose them? 

## Evaluate your choices
Use profiling and benchmarking to evaluate which of your options is likely to be the fastest. How does the syntax and/or ease of use of that function impact your decision of whether or not to use it? (For example, velox is much faster than raster, but it's less well documented and the syntax is strange to get used to).

## Show us your final product
Did you make a map? Let's see it. Did you plot some data that you extracted with raster? show us that plot. Did you have an idea of how the data should look after you were done processing it? Were you successful? What went wrong

## Reflect
Write a few sentences on what you learned from this exercise. How has your skill improved? What do you wish you understood better? What do you imagine your next steps to be?

Once you're done push the "knit" button to create the html page from your Rmarkdown document. If you've got questions, let me know!!