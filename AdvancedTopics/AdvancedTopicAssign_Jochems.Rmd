---
title: "Advanced Topic Instructions"
author: "Louis Jochems"
date: "3/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(root.dir = "C:/Users/LouisJochems/Documents/HES598/datatemp")
```


SAVE YOUR FILE IN THE AdvancedTopics FOLDER IN THE GIT REPO

## Introduce your problem

Give a brief introduction to your research and identify how your "Advanced Topic" plays into what you are hoping to do. For example, if you are just starting out on your project and your topic is "Study Design" you might be interested in developing a spatially informed sampling/stratification scheme.Or maybe your work is going to have a big public outreach component that you'd like to support with some nice interactive maps. Give us a sense for what "success" would look like - what is your ideal end product for the analysis/approach you're testing out. This will help us figure out if the tools you tried were successful.

This chapter of my dissertation involves detecting and predicting the spread (or really, the range/distribution) of a newly invading macrophyte (Hydrocharis morsus-ranae L; EFB) in Great Lakes wetlands. Because EFB is not yet fully established in all Great Lakes, I am investigating whether we can accurately predict vulnerable wetlands based on EFB's environmental/anthropogenic drivers. The importance of these hypothesized drivers (eg. distance to public boat launch, the presence of an emergent macrophyte, etc.) can be assessed through various species distribution models. A species distribution model can have varying definitions, but I interpret it as a representation of the influence of ecological, climatic, and anthropogenic variables on whether or not a species' presence/abundance is observed across time and space. In other words, understanding which variables are important for EFB occurence at its known locations can help us predict the full potential range or habitat of EFB (based on the similarity of uninvaded wetlands variables to sites of known occurence) if it were to invade the entire GL basin. Overall, these questions are important to address so that managers can properly control/mitigate EFB's spread and priortize protection of vulnerable wetlands to future invasions. Spatio-temporal data play a huge role in answering my questions because predicting EFB's current and potential distribution in its invasive range requires spatial data ideally over decadal timescales.

Specifically for my research, I have a obtained access to a relatively long time-series field dataset, consisting of 8 years of transect vegetaion survey data in Michigan coastal wetlands (2011-2018). It includes plant species occurence/abundance, environmental data (water depth, distance to wave action, water quality, etc.), GPS coordinates of quadrats, and date at which the data was collected.

For this advanced topic, I'll consider this exercise a success by cleaning/wrangling the dataset as it was given to me, and then exploring a few different modeling approaches for predicting EFB's distribution. I will load in a topobathymetry layer from NOAA lidar surveys (proxy for water depth) and then extract values from this raster to the GPS point locations for EFB's occurrence in the study area. I will then plot these models' predictions and assess the validity of the model outputs. 

## PSUEDOCODE!!
Before you get started programming all of the potential things you could do during the course of this demo. Lay out the steps you'll need to get there (see below for what I mean). You don't have to actually code these things (yet) just help us see how you're approaching the problem.
```{r}
#load the libraries - tell us which packages you're using and why
knitr::opts_chunk$set(echo = TRUE)
library(googledrive)
library(curl)
library(here)
# To connect to folder with field data. 
library(tidyverse)
library(reshape2)
#I need tidyverse and reshape2 so that I can rearrange (via the melt function) my dataset into a format that is analyzable. Specifically, the field data that was given to me consisted of individual species observations, each as their own row (so like, >70k rows!). Instead, I would like each row to be a given quadrat plot with all spp observed and recorded variables. tidyr's spread() function gets the dataframe into the desired format. 

library(sf)
library(sp)
library(rgdal)
library(raster)
library(spatialEco)
#These libraries help me eventually turn my dataframe with lat/lons into a spatial dataframe (eventually will plot all over the state of MI). Each point is a surveyed quadrat in a given wetland. In particular, the raster() package allows me to display the toppobathymetry layer as well as use various SDM's predictions to map the probability of EFB across the raster layer. 

library(maps)
library(mapproj)
library(tmap)
library(maptools)
#These packages have functions to ensure my various geospatial layers have the same projection, and can make my maps look pretty! 

library(ggplot2)
library(patchwork)
library(geofacet)
#ggplot2 gives me the flexibility to make my plots more informative (see above about including multiple attributes), and patchwork will potentially allow me to plot multiple panel plots to compare and contrast patterns across different variables. Lastly, geofacet will help me create plots of processes organized by different geographic subsets in the state (county, great lake, geomorphic subset etc)

library(RColorBrewer)
library(latticeExtra)
library(RColorBrewer)
library(viridis)
#library(rJava)
#For making my plots looks look pretty with nice color palletes. Might be useful for some of my variables that are on different scales. 

library(dismo)
# For this exercise, I will use the dismo package to test the applicability of a few its functions/models to assess and map EFB suitability (in terms of water depth). 

#library(INLA)
#library(inlabru)
#Finally, INLA allows me to create a species distribuion model (in a Bayesian framework) with a spatial/temporal autocorrelation terms relative to the effects of the hypothesized predictor variables. I can also plot the spatio-temporal effects of the variables across MI coastlines (ie. where/when EFB occurence exhibits the highest spatio-temporal autocorrelation). Eventually (though probably not in the scope of this exercise), I would like to plot the risk of uninvaded wetlands to potential EFB establishment. This would be based on the model estimates of the predictor's posterior distributions. 

library(microbenchmark) 
library(parallel)
#While I am preemptively subsetting the spatial scale and data here, I will benchmark an extrapolation of how long an analysis may take for the whole state of MI (e.g. raster::predict() function on a large topobathymetry layer) 
#Lastly, although the INLA framework allows for high computation of variables with complex structure through stochastic partial differential equations, I still likely need to assess how long these models will take to run given this relatively long time-series of field data. 
```

Processing steps below: 
Ignore this chunk, I largely did this through a working directory from another script
Will leave here for reference on data wrangling/organization 

```{r}
# #load your data - which datasets do you need?
# #setwd("Z:/SDMData/FullVegDataset2011-18")
# veg_raw <- read.csv("FullVegDataset2011-18.csv")
# m.raw <- melt(veg_raw, id=c("site_id","county","date","year","site_name","transect_num","meadow_width_m","emergent_width_m","submergent_width_m",
#                             "point_num","gps_lon","gps_lat","water_depth_cm","bottom_vis","substrate_type","org_depth_cm","unvegetated_pcnt",
#                             "total_cover_pcnt","standing_dead_pcnt","detritus_pcnt", "taxa_name"),
#               measure.vars = c("percent_cover"))
# 
# m.raw <- m.raw[m.raw$taxa_name %in% c("Hydrocharis morsus-ranae","Typha angustifolia","Typha glauca","Typha glauca (hybrid)","Typha latifolia","Typha sp.",
#                                    "Phragmites australis","Phragmites australis (invasive","Phragmites australis (native)"),]
# #This is the full field dataset of vegetation surveys of Michigan wetlands from 2011-2018. This includes many spp observations, but I've reduced it to EFB, Typha, and Phragmites. 
# 
# #Organize the data - what form should the data be in? A list? how many elements, a data frame? how many rows and columns?
# veg_data <- m.raw %>% spread(taxa_name, value)
# #replace NA's with o 
# veg_data[is.na(veg_data)] <- 0
# 
# #Confusion with distinguishing typha glauca and the hybrid in the field, so I will combine them
# veg_data$typha_combined <- veg_data$`Typha glauca` + veg_data$`Typha glauca (hybrid)` + veg_data$`Typha sp.`
# veg_data <- data.frame(veg_data)
# 
# #create binomial presence/absence data (needed for SDMs down the road)
# veg_data$hyd_bin <- ifelse(veg_data$Hydrocharis.morsus.ranae>0,1,0)
# veg_data$typh_bin <- ifelse(veg_data$typha_combined>0,1,0)
# 
# dim(veg_data)
# #[1] 2255   30
# #2,225 rows with 30 columns of data. Each row is a quadrat from a given transect in a given year between 2011-18. 
# 
# #First, I need to create a shape file for my full quadrat layer 
# #for now, subset to quads that have coords 
# veg_data <- veg_data[which(veg_data$gps_lon < 0 & veg_data$gps_lat>0),]
# # Many quadrats without GPS coordinates unfortunately 
# # Now we only 1873 plots total
# 
# 
# #Take the full_veg dataset and subset to counties with records that overlap our topobathymetry layer
# #Will work with Munuscong Bay layer in the Upper Peninsula (UP)
# UP_data <- as.data.frame(veg_data)
# 
# #Check for duplicates of GPS coordinates 
# dups <- UP_data[duplicated(UP_data[,c(11,12)]),c(11,12)]
# #none!
# 
# #also see if there are errors with gps coords 
# range(UP_data$gps_lon)
# range(UP_data$gps_lat)
# 
# #Since our lidar layer is even a smaller subset of the area, we need to further subset points to the ROI, around Munuscong Bay on the St. Mary's River (technically Lake Huron)
# #Will do so by site names, determined from GIS
# UP_data$site_name <- as.factor(UP_data$site_name)
# Mcong <- UP_data[UP_data$site_name %in% c("Benchmark: Munuscong Island Wetland", "Benchmark: Munuscong Lake Wetland #6","Gogomain River Wetland #1","Munuscong Lake Wetland #2,#3 Munuscong River Delta","Raber Bay Wetland" ,"Roach Point Wetland"),] 
# #remove redundant GPS columns 
# Mcong <- within(Mcong, rm(gps_lon.1, gps_lat.1))
# 
# #write.csv(Mcong,"UP_data.csv")
# 
# #let's make dataframe into spatial object with binomial occurence 
# # now need to create sp dataframe 
# #Mcong <- SpatialPointsDataFrame(Mcong[,11:12],Mcong)
# #This new SP dataframe has no coordinate reference system, and normally I would assign UTM in other applications, but I will wait until I load in this specific raster dem. 
# #crs(Mcong) <- "+proj=utm +zone=16 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0"
# # plot(Mcong)

```

```{r} 
#will load in subsetted data from class's google drive 
# folder.url <- "https://drive.google.com/open?id=1cjqJMM2JyQWDtVorNEyO6HLprB_s7XOF" #replace this link with the link to your specific folder which you can get by clicking the "get shareable link option"
# 
# folder <- drive_get(as_id(folder.url))
# # 
# # gdrive_files <- drive_ls(folder)
# 
# lapply(gdrive_files$id, function(x)drive_download(as_id(x), 
#                                         path = paste0(here::here("/datatemp/original/"), 
#                                             gdrive_files[gdrive_files$id==x,]$name), overwrite = TRUE)) 
# # #replace the path with a folder on your computer outside of the repo directory
# 
# file.list <- list.files((pattern = "*.csv")) #replace the pattern with the appropriate extensions for the files you want to load (e.g., .shp, .csv)
# field_data <- lapply(file.list, read.csv) #Then read in the file using the appropriate read command (e.g., raster, read_sf, read.csv)
# #for DEM in folder as well 
# file.list <- list.files((pattern = "*.tif")) 
# topobath <- lapply(file.list,raster)

setwd("C:/Users/LouisJochems/Documents/HES598/datatemp/")
field_data <- read.csv("C:/Users/LouisJochems/Documents/HES598/datatemp/originalUP_data.csv")
topobath <- raster("C:/Users/LouisJochems/Documents/HES598/datatemp/originalMcongStMarysRiver.tif")

```


More data to load!
This topobathymetry layer is a digital elevation models (DEM) from NOAA's Digital Coast repository, where they have collected aerial lidar of all US coastlines, including the Great Lakes. The lidar uses green/NIR lasers to penetrate into the water surface, and thus are able to retrieve returns from the bottom in the form of a DEM (depending on turbidity of course). The extent of these surveys are fairly restricted to the coasts, and thus I am making an assumption that most of the areas are indeed GL water surfaces/wetlands, or potentially suitable habitat for EFB.

```{r}
#rough name, but that's how NOAA (or US Army Corps of Engineers) delivers requested area.
# I renamed the the .tif file in the google drive to McongStMarysRiver, where the lidar covers Munsucong Bay along the St. Mary's river 
#topobath <- raster("Job534911_2013_2015_usace_huron_dem.tif")

# Although I just have one raster layer, I assigned the layer as a rasterstack for the modeling analyses below. 
topobath <- stack(topobath)

# Unfortunately, the raster is in a lambert conical projection in NAD 1983 (with units in feet?!), but we often don't want to reproject a raster as it can skew the cells. Therefore, I make the decision here to project the points layer into the NAD1983 projection of the DEM. 
#CRS of topobathymetry layer 
crs(topobath) <- "+init=epsg:2251 +proj=lcc +lat_1=47.08333333333334 +lat_2=45.48333333333333 +lat_0=44.78333333333333 +lon_0=-87 +x_0=7999999.999968001 +y_0=0 +datum=NAD83 +units=ft +no_defs +ellps=GRS80 +towgs84=0,0,0"


#After using the crs() function on the UP_data points dataframe, there was a misalignment between the points and raster when I plotted them. Instead, I tried sptransform() below and achieved the correct alignment in the plot. 
cord.dec <- SpatialPointsDataFrame(field_data[,12:13], field_data, proj4string =CRS("+proj=longlat"))
cord.NAD <- spTransform(cord.dec,CRS("+init=epsg:2251"))

plot(topobath)
points(cord.NAD,cex=0.5)
#points intersect with layer! 
```

```{r}
#extract values from topobath layer to serve as a predictor variable for EFB presence 
predictor <- raster::extract(topobath,cord.NAD)

topo_depth <- as.vector(predictor)

#Add vector of predictor values to our spatial data frame 
#Remember to keep name consistent with that of original raster layer so that we can map predictions later on! 
cord.NAD$originalMcongStMarysRiver <- topo_depth

#glm will remove obs with NAs values, but for clealiness' sake, let's remove all transect survey plots in the rest of the county (those that did not intersect with the topobathymetry layer)
cord.NAD_mod <- sp.na.omit(cord.NAD, col.name = "originalMcongStMarysRiver")

#Plot to check values extracted from raster
plot(hyd_bin~originalMcongStMarysRiver,data=cord.NAD, xlab="Elevation (ft)", ylab="EFB Occurrence")

#now we are ready to run a glm between topo_depth 
#binomial model for EFB occurence against topo_depth 
m1 <- glm(hyd_bin~originalMcongStMarysRiver, data=data.frame(cord.NAD), family= binomial(link="logit"))
```


```{r}
#Check the outcomes? How will you know if your steps worked?
summary(m1)

#plot model predictions on entire extent of topobathymetry layer 
p <- raster::predict(topobath, m1, progress="text")
#Takes awhile, so let's say I wanted to predict EFB occurence across multiple rasters throughout the state. I'll use microbenchmark to get a rough idea of just how long that could take 

# predict.mb <- microbenchmark(p,times=1)
#this takes about 2 minutes for a dataset that spans about 25km, so extrapolating this out for the entire state coastlines of ~5300 km, could take 10,600 minutes, or 176 hours! Definitely will need a super comp... 

#map of proability of EFB occurence based on topobathymetry dem 
plot(p)

#I need to plogis cell values to get probabilities on real numbers scale that is actually interpretable
p2 <- calc(p,plogis)
plot(p2, main= "Probability of EFB Occurence across Topobathymetry DEM, Munsucong Bay, St. Mary's River, MI")

#Here, we can see that, on a real numbers scale, the areas with lower elevation (deeper, open water) predict high probability of EFB occurence. We can also see that as the DEM elevation increases onto likely terrestrial areas, the probability of occurence decreases. There is variation in EFB probability of occurence in the areas in between, e.g. the coastal marshes. A major issue to consider is that this topobathymetry is just one variable, while it is likely multiple processes that influence EFB occurence in GL wetlands. 
```

```{r}
#Given these findings, there are still a few things I should do to assess whether a glm (regression) model or alternative models (in dismo and beyond) may be more suitable for predicting EFB occurrence, given these datasets, in this region. 
nrow(cord.NAD_mod)
which(cord.NAD_mod$hyd_bin == 0)
#113 absences out of 162 survey points
#49 presences, so only 30% of datapoints document EFB occurence 

#We need to evaluate train a model with 4/5's of the data set, and evaluate it with the remaining 1/5 of data. We can easily do this with the dismo's kfold partinioning method and evaluate(). 
cord.NAD_mod <- data.frame(cord.NAD_mod)
pres <- cord.NAD_mod[cord.NAD_mod[,31] == 1, 35]
abs <- cord.NAD_mod[cord.NAD_mod[,31] == 0, 35]

evaluate(pres,abs,m1)

## split data into a train and test set
index <- 1:nrow(cord.NAD_mod)
testindex <- sample(index, trunc(length(index)/5))
testset <- cord.NAD_mod[testindex,]
trainset <- cord.NAD_mod[-testindex,]

pres_train <- trainset[which(trainset$hyd_bin == 1),]
pres_test <- testset[which(testset$hyd_bin == 1),]

abs_train <- trainset[which(trainset$hyd_bin == 0),]
abs_test <- testset[which(testset$hyd_bin == 0),]


m2 <- glm(hyd_bin~Job534911_2013_2015_usace_huron_dem, data=trainset, family= binomial(link="logit"))
evaluate(pres_test,abs_test,m2)

#I should also consider kfold() cross validation, where I train the model on 4/5 of my (spatially blocked) data and test it with 1/5, and cycle thru that multiple times. Further step down the road! 

#While AUC is a controversial metric for evaluating species distribution models (AUC varies with spatial extent of data), I can also assess and remove the spatial sorting bias (difference between distance from testing presence to training presence and distance from testing-abs to training-presence points)
#ssb only takes two column matrices with long/lat for presence and absesnce points 

p_testsub <- subset(pres_test, select = c("gps_lon","gps_lat"))
a_testsub <- subset(abs_test, select = c("gps_lon","gps_lat"))
p_trainsub <- subset(pres_train, select = c("gps_lon","gps_lat"))
sb <- ssb(p_testsub,a_testsub,p_trainsub)
sb[,1]/sb[,2]
#values close to one would indicate no ssb and close to zero would indicate high ssb. Here we have an ssb of 0.022, not great! 
#No fear, we can use pwdSample() function in dismo to select paris of points from two sets (without replacement) that should have a similar distance to nearest point in another set of points. 
i <- pwdSample(p_testsub,a_testsub,p_trainsub, n=1, tr=0.1)
pres_test_pwd <- p_testsub[!is.na(i[,1]), ]
back_test_pwd <- a_testsub[na.omit(as.vector(i)), ]
sb2 <- ssb(pres_test_pwd, back_test_pwd, p_trainsub)
sb2[,1]/ sb2[,2]
#Now, 1.02, there is no spatial sorting bias! I will apply this approach to a future model. 
```

```{r}
#For this project, I will attempt one more two of modeling appraoch and compare the outcomes to the glm. The rest of the modeling functions in dismo often deal with presence only data, so I will instead use the machine learning randomForest() regression one the presence/absence data that I have.  
library(randomForest)
model <- hyd_bin~McongStMarysRiver
rf1 <- randomForest(model,data=trainset)
erf <- evaluate(pres_test,abs_test,rf1)
#Without spatial blocking, AUC is slightly improved to 0.62. 

#now let's plot RF predictions across the entire raster 
prf <- raster::predict(topobath, rf1, progress='text')
plot(prf, main= "Probability of EFB Occurence across Topobathymetry DEM, Munsucong Bay, St. Mary's River, MI")
```


```{r}
#EFB's habitat suitability from rf predictions look quite different from the glm! Lastly, we can plot individual outputs and compare side by side, and also average the model ouputs and plot EFB habitat suitability. 
models <- stack(p2, prf)
names(models) <- c("GLM", "RF")
plot(models)
#Compare side by side

#average predictions 
m <- mean(models)
plot(m, main= "Average Score")
```
## Introduce the packages
Given your psuedo code, where is the critical step? What packages and functions are you considering to help you complete this step? Why did you choose them? 
The critical steps here were to reorganize the dataset into an analyzable format (survey plots as rows, with species occurence/ predictors variables as columns) which I was able to accomplish with tidyr. Also, I needed to subset the entire field dataset to the county where the topobathymetry layer is located, with base r. I then needed to convert this dataset into a SpatialPoints dataframe and assign it's CRS to that of the DEM. Lastly, I needed to extract values from the raster and then use these as predictors for my occurrence points in the models. Lastly, raster::predict allowed me to predict the probability of EFB occruence across all cells of the raster based on the estimates from the model outputs. 

## Evaluate your choices
Use profiling and benchmarking to evaluate which of your options is likely to be the fastest. How does the syntax and/or ease of use of that function impact your decision of whether or not to use it? (For example, velox is much faster than raster, but it's less well documented and the syntax is strange to get used to).
Chunk 8! 

## Show us your final product
Did you make a map? Let's see it. Did you plot some data that you extracted with raster? show us that plot. Did you have an idea of how the data should look after you were done processing it? Were you successful? What went wrong
Chunk 7 for extracted topobathymetry values, and Chunks 9-11 for somewhat informative maps!  

## Reflect
Write a few sentences on what you learned from this exercise. How has your skill improved? What do you wish you understood better? What do you imagine your next steps to be?
I've learned that there are many ways to model and predict the distribution of a species! I've also learned that a spatially unbiased field data set of occurence is likely key for me to most optimally predict the distribution of EFB. Since my dataset and predictors is fairly spatially biased, I have learned that there are ways to reduce those biases in my training and testing datasets to obtain better estimates of predictors in whatever modeling approach I use. I also learn that scale matters. This topobathymetry layer I used for this project is on the scale of feet, whereas EFB's true distribution may vary more on the scale of cm within and across the wetland vegetation zones. Lastly, like in the dismo vignette, there are likely many biophysical and anthropogenic factors that contribute to EFB's distribution in MI! If available, I should consider radar rasters (for emergent vegetation) and other geospatial datasets (e.g. nutrient levels) to incluede in my SDM framework. 

Once you're done push the "knit" button to create the html page from your Rmarkdown document. If you've got questions, let me know!!