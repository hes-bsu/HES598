---
title: "Advanced Topic - Landscape Metrics and Raster Data"
author: "Allison Vincent"
date: "4/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introducing the problem

My thesis research involves using a remote sensing data fusion model to monitor snow covered area in a mountain watershed. The questions we are trying to answer include:

1. Can we constrain the spatiotemporal distribution of snow cover in mountain watersheds over a large area and long period of time to better understand water delivery to the Critical Zone (or shallow subsurface)?

2. Can we create a high spatiotemporal resolution dataset to monitor snow covered area in a mountain watershed over a ~20 year period?

To do this, we are using the Spatial and Temporal Adaptive Reflectance Fusion Model (STARFM) to downscale MODIS (low spatial, high temporal resoultion) data to fill in the gaps of Landsat (high spatial, low temporal resolution) data to monitor historical snow-covered area in detail on a daily basis. Landsat data with its 30 meter spatial resolution is useful for detailed monitoring of landscape features. Because it is only available every 7-16 days (depending on location), however, rapidly changing conditions, such as spring snowcover, can be missed. On the other hand, MODIS data is captured on a daily basis, but is too coarse at 250-500 meter spatial resolution to be used to discerne landscape changes at the scale necessary. Downscaling MODIS data is one way to resolve this issue. By using MODIS data to "fll-in" the gaps between days of Landsat acquisition, a dataset can be produced that contains synthetic imagery at a Landsat-like spatial resolution on a daily basis. This is important because snowcover is a landscape characteristic that can change rapidly from day-to-day, especially during times such as the spring snowmelt season.    

Before being used as model input, MODIS and Landsat images are first processed by applying a cloud mask to remove any pixels that are classified as cloud or cloud shadow. The normalized difference snow index (NDSI) is then calculated for the remaining pixels covering the area of interest. Once an NDSI value for each pixel is obtained, both images are converted to the same projection and the MODIS imagery is rescaled to 30x30 meter pixels.

To validate the results of the model, a single Landsat image is excluded from the model input data, and then later is used to compare the synthetic image produced from the model from that same date to see how accurate the model's prediction was. One way in which we quantify the performance of the model is by binary classification. We use the common NDSI threshold of 0.4 to decide whether a pixel is classified as snow or no snow. Any value at 0.4 or above is given the label of "snow" (the positive classification), and any value below is given the label of "no snow" (the negative classification). 

Once each pixel is placed into one of these categories, a confusion matrix of true positives, true negatives, false positives, and false negatives is generated. From these values we then calculate metrics such as accuracy, precision, recall, and F-score for each date. These last 3 metrics are used because, as our dates of analysis move further into the spring, snow becomes more scarce in the landscape and the negative, "no-snow", class becomes the majority classifier, which can inflate the accuracy values for even an unskilled model. (add more details and equations for these metrics if necessary)

In addition to the calculation of the F-score for the model output, it is also important to quantify how well the model is performing against a random distribution of snowcover. This project proposes to do just that. Below is a draft outline of the steps I hope to accomplish in this analysis:

1. Choose a date for analysis that is less than 30% cloud covered. 

2. Calculate the fractional snow-covered area (fSCA) of the Landsat image (the validation image) for that date as the fraction of pixels classified as "snow" (the positive class). 

3. Generate a map of the modeled data from that same date with the same fSCA, but with the snowcover randomly distributed throughout the landscape. 

4. Repeat step 3 multiple times (100-1,000? depending on computing power available and time-burden), producing an ensemble of random binary snow-covered area maps.

5. Calculate the accuracy, precision, recall, and F-score of all above random simulations.

6. Produce a figure that shows how the values from step 5 are different from the true values of the original real/synthetic image pair. This step can include measures of the variance in the values calculated from the random trials from the expected values calculated from the model output.  


## PSUEDOCODE!!
Before you get started programming all of the potential things you could do during the course of this demo. Lay out the steps you'll need to get there (see below for what I mean). You don't have to actually code these things (yet) just help us see how you're approaching the problem.
```{r}
#load the libraries - tell us which packages you're using and why
#load your data - which datasets do you need?
#Organize the data - what form should the data be in? A list? how many elements, a data frame? how many rows and columns?
#Analysis/processing step 1 - what are you hoping to do here, why? 
#Analysis/processing step 2 - what are you hoping to do here, why? 
#Check the outcomes? How will you know if your steps worked?
# 
```

## Introduce the packages
Given your psuedo code, where is the critical step? What packages and functions are you considering to help you complete this step? Why did you choose them? 

## Evaluate your choices
Use profiling and benchmarking to evaluate which of your options is likely to be the fastest. How does the syntax and/or ease of use of that function impact your decision of whether or not to use it? (For example, velox is much faster than raster, but it's less well documented and the syntax is strange to get used to).

## Show us your final product
Did you make a map? Let's see it. Did you plot some data that you extracted with raster? show us that plot. Did you have an idea of how the data should look after you were done processing it? Were you successful? What went wrong

## Reflect
Write a few sentences on what you learned from this exercise. How has your skill improved? What do you wish you understood better? What do you imagine your next steps to be?

Once you're done push the "knit" button to create the html page from your Rmarkdown document. If you've got questions, let me know!!
